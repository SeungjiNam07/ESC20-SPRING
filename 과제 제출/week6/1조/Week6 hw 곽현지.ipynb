{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week6: PCA & FA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.precision',3)\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1     2     3     4    5     6     7     8     9     10    11    12  \\\n",
       "0   1  14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29  5.64  1.04  3.92   \n",
       "1   1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28  4.38  1.05  3.40   \n",
       "2   1  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81  5.68  1.03  3.17   \n",
       "3   1  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18  7.80  0.86  3.45   \n",
       "4   1  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82  4.32  1.04  2.93   \n",
       "\n",
       "     13  \n",
       "0  1065  \n",
       "1  1050  \n",
       "2  1185  \n",
       "3  1480  \n",
       "4   735  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = pd.read_csv(r'C:\\Users\\njj06\\Downloads\\wine.csv', header = None) #14x14, 열 이름 포함되지 않은 데이터\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malicAcid</th>\n",
       "      <th>ash</th>\n",
       "      <th>ashalcalinity</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>totalPhenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonFlavanoidPhenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>colorIntensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280_od315</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name  alcohol  malicAcid   ash  ashalcalinity  magnesium  totalPhenols  \\\n",
       "0     1    14.23       1.71  2.43           15.6        127          2.80   \n",
       "1     1    13.20       1.78  2.14           11.2        100          2.65   \n",
       "2     1    13.16       2.36  2.67           18.6        101          2.80   \n",
       "3     1    14.37       1.95  2.50           16.8        113          3.85   \n",
       "4     1    13.24       2.59  2.87           21.0        118          2.80   \n",
       "\n",
       "   flavanoids  nonFlavanoidPhenols  proanthocyanins  colorIntensity   hue  \\\n",
       "0        3.06                 0.28             2.29            5.64  1.04   \n",
       "1        2.76                 0.26             1.28            4.38  1.05   \n",
       "2        3.24                 0.30             2.81            5.68  1.03   \n",
       "3        3.49                 0.24             2.18            7.80  0.86   \n",
       "4        2.69                 0.39             1.82            4.32  1.04   \n",
       "\n",
       "   od280_od315  proline  \n",
       "0         3.92     1065  \n",
       "1         3.40     1050  \n",
       "2         3.17     1185  \n",
       "3         3.45     1480  \n",
       "4         2.93      735  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.columns = ['name','alcohol','malicAcid','ash','ashalcalinity','magnesium',\n",
    "                'totalPhenols','flavanoids','nonFlavanoidPhenols','proanthocyanins',\n",
    "                'colorIntensity','hue','od280_od315','proline']\n",
    "wine.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name                   0\n",
       "alcohol                0\n",
       "malicAcid              0\n",
       "ash                    0\n",
       "ashalcalinity          0\n",
       "magnesium              0\n",
       "totalPhenols           0\n",
       "flavanoids             0\n",
       "nonFlavanoidPhenols    0\n",
       "proanthocyanins        0\n",
       "colorIntensity         0\n",
       "hue                    0\n",
       "od280_od315            0\n",
       "proline                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결측치 확인 (NA) -> 없다.\n",
    "wine.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malicAcid</th>\n",
       "      <th>ash</th>\n",
       "      <th>ashalcalinity</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>totalPhenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonFlavanoidPhenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>colorIntensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280_od315</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>178.000</td>\n",
       "      <td>178.000</td>\n",
       "      <td>178.000</td>\n",
       "      <td>178.000</td>\n",
       "      <td>178.000</td>\n",
       "      <td>178.000</td>\n",
       "      <td>178.000</td>\n",
       "      <td>178.000</td>\n",
       "      <td>178.000</td>\n",
       "      <td>178.000</td>\n",
       "      <td>178.000</td>\n",
       "      <td>178.000</td>\n",
       "      <td>178.000</td>\n",
       "      <td>178.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.938</td>\n",
       "      <td>13.001</td>\n",
       "      <td>2.336</td>\n",
       "      <td>2.367</td>\n",
       "      <td>19.495</td>\n",
       "      <td>99.742</td>\n",
       "      <td>2.295</td>\n",
       "      <td>2.029</td>\n",
       "      <td>0.362</td>\n",
       "      <td>1.591</td>\n",
       "      <td>5.058</td>\n",
       "      <td>0.957</td>\n",
       "      <td>2.612</td>\n",
       "      <td>746.893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.775</td>\n",
       "      <td>0.812</td>\n",
       "      <td>1.117</td>\n",
       "      <td>0.274</td>\n",
       "      <td>3.340</td>\n",
       "      <td>14.282</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.572</td>\n",
       "      <td>2.318</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.710</td>\n",
       "      <td>314.907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000</td>\n",
       "      <td>11.030</td>\n",
       "      <td>0.740</td>\n",
       "      <td>1.360</td>\n",
       "      <td>10.600</td>\n",
       "      <td>70.000</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.410</td>\n",
       "      <td>1.280</td>\n",
       "      <td>0.480</td>\n",
       "      <td>1.270</td>\n",
       "      <td>278.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000</td>\n",
       "      <td>12.362</td>\n",
       "      <td>1.603</td>\n",
       "      <td>2.210</td>\n",
       "      <td>17.200</td>\n",
       "      <td>88.000</td>\n",
       "      <td>1.742</td>\n",
       "      <td>1.205</td>\n",
       "      <td>0.270</td>\n",
       "      <td>1.250</td>\n",
       "      <td>3.220</td>\n",
       "      <td>0.782</td>\n",
       "      <td>1.938</td>\n",
       "      <td>500.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000</td>\n",
       "      <td>13.050</td>\n",
       "      <td>1.865</td>\n",
       "      <td>2.360</td>\n",
       "      <td>19.500</td>\n",
       "      <td>98.000</td>\n",
       "      <td>2.355</td>\n",
       "      <td>2.135</td>\n",
       "      <td>0.340</td>\n",
       "      <td>1.555</td>\n",
       "      <td>4.690</td>\n",
       "      <td>0.965</td>\n",
       "      <td>2.780</td>\n",
       "      <td>673.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000</td>\n",
       "      <td>13.678</td>\n",
       "      <td>3.083</td>\n",
       "      <td>2.558</td>\n",
       "      <td>21.500</td>\n",
       "      <td>107.000</td>\n",
       "      <td>2.800</td>\n",
       "      <td>2.875</td>\n",
       "      <td>0.438</td>\n",
       "      <td>1.950</td>\n",
       "      <td>6.200</td>\n",
       "      <td>1.120</td>\n",
       "      <td>3.170</td>\n",
       "      <td>985.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000</td>\n",
       "      <td>14.830</td>\n",
       "      <td>5.800</td>\n",
       "      <td>3.230</td>\n",
       "      <td>30.000</td>\n",
       "      <td>162.000</td>\n",
       "      <td>3.880</td>\n",
       "      <td>5.080</td>\n",
       "      <td>0.660</td>\n",
       "      <td>3.580</td>\n",
       "      <td>13.000</td>\n",
       "      <td>1.710</td>\n",
       "      <td>4.000</td>\n",
       "      <td>1680.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name  alcohol  malicAcid      ash  ashalcalinity  magnesium  \\\n",
       "count  178.000  178.000    178.000  178.000        178.000    178.000   \n",
       "mean     1.938   13.001      2.336    2.367         19.495     99.742   \n",
       "std      0.775    0.812      1.117    0.274          3.340     14.282   \n",
       "min      1.000   11.030      0.740    1.360         10.600     70.000   \n",
       "25%      1.000   12.362      1.603    2.210         17.200     88.000   \n",
       "50%      2.000   13.050      1.865    2.360         19.500     98.000   \n",
       "75%      3.000   13.678      3.083    2.558         21.500    107.000   \n",
       "max      3.000   14.830      5.800    3.230         30.000    162.000   \n",
       "\n",
       "       totalPhenols  flavanoids  nonFlavanoidPhenols  proanthocyanins  \\\n",
       "count       178.000     178.000              178.000          178.000   \n",
       "mean          2.295       2.029                0.362            1.591   \n",
       "std           0.626       0.999                0.124            0.572   \n",
       "min           0.980       0.340                0.130            0.410   \n",
       "25%           1.742       1.205                0.270            1.250   \n",
       "50%           2.355       2.135                0.340            1.555   \n",
       "75%           2.800       2.875                0.438            1.950   \n",
       "max           3.880       5.080                0.660            3.580   \n",
       "\n",
       "       colorIntensity      hue  od280_od315   proline  \n",
       "count         178.000  178.000      178.000   178.000  \n",
       "mean            5.058    0.957        2.612   746.893  \n",
       "std             2.318    0.229        0.710   314.907  \n",
       "min             1.280    0.480        1.270   278.000  \n",
       "25%             3.220    0.782        1.938   500.500  \n",
       "50%             4.690    0.965        2.780   673.500  \n",
       "75%             6.200    1.120        3.170   985.000  \n",
       "max            13.000    1.710        4.000  1680.000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler #for scaling\n",
    "\n",
    "X = wine.iloc[:,1:] \n",
    "y = wine.iloc[:,0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 123, stratify = y)\n",
    "# stratify=y : 나눠진 데이터도 원자료의 비율을 따른다. 여기서는 y의 class 비율.\n",
    "\n",
    "# PCA할때는 scaling 꼭 해줘야됨!\n",
    "X_train_scaled = StandardScaler().fit_transform(X_train)\n",
    "X_test_scaled = StandardScaler().fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.892679500939597,\n",
       " 2.4608503376537256,\n",
       " 1.5230183470998415,\n",
       " 0.952753904378899,\n",
       " 0.8313412106403766,\n",
       " 0.5678215651286187,\n",
       " 0.5009709173547725,\n",
       " 0.37366049730166373,\n",
       " 0.2679129458257205,\n",
       " 0.2556366468100889,\n",
       " 0.20175528788966957,\n",
       " 0.17818150436652608,\n",
       " 0.09910839152108603]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eigenvalues and eigenvectors\n",
    "# eigenvalue의 크기 순으로 재배열 해줄거야\n",
    "\n",
    "X_train_scaled = pd.DataFrame(data=X_train_scaled, columns=wine.columns[1:])\n",
    "cov_mat = np.cov(X_train_scaled.T)\n",
    "eig_vals, eigvecs = np.linalg.eig(cov_mat)\n",
    "eig_vals_s = sorted(eig_vals, reverse=True)\n",
    "eig_vals_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1b3a70a20f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHTZJREFUeJzt3Xt83HWd7/HXZ2ZyzyRNpml6SUuaAqWlLVSiJKC7irIPFlRAuytn1eLjKFXRXXf1HFD37HqQlePCPpbVh+seKCKyIOpyOQriBRHXRdpCCvTOtS1tek3a3G+TmfmeP2YSeknIJJnJby7v5+ORx1w6zLwHyrvffub7+4055xARkezh8zqAiIhMjopbRCTLqLhFRLKMiltEJMuouEVEsoyKW0Qky6i4RUSyjIpbRCTLqLhFRLJMIB1POnv2bFdfX5+OpxYRyUmbN29ud87VJPPYtBR3fX09LS0t6XhqEZGcZGZvJPvYpIrbzPYCPUAUiDjnGqcWTUREpmsyK+73OOfa05ZERESSog8nRUSyTLLF7YBfm9lmM1uXzkAiIvLWkh2VXOycO2hmc4AnzOwl59zvT3xAotDXASxatCjFMUVEZERSK27n3MHE5VHgEeAdYzzmTudco3OusaYmqR0tJ4nFHG09Qxzo6KetZ4hYTF/wICIylglX3GZWBviccz2J638CfD2VIWIxx8tHerju3hZaOwaoqyph/dpGltYG8fkslS8lIpL1kllx1wJPm9kW4Fng5865X6YyxLG+8GhpA7R2DHDdvS0c6wun8mVERHLChCtu59xu4Lx0hghHoqOlPaK1Y4BwJJrOlxURyUoZsR2wMOCnrqrkpPvqqkooDPg9SiQikrkyorhDZYWsX9s4Wt4jM+5QWaHHyUREMk9azlUyWT6fsbQ2yAPXNXGwc4CKkgJ9MCkiMo6MWHFDvLzrqkr4/AMvcMd/vq7SFhEZR8YUN4CZ0dQQYsPuYzinfdwiImPJqOIGaGqo5kj3EHva+7yOIiKSkTKuuJsbQgBs2H3M4yQiIpkp44p78ewyaiuK2Lj7uNdRREQyUsYVt5nR3BBiw+uac4uIjCXjihugqSFEe+8Qr7f1eh1FRCTjZGRxNy9JzLlf15xbRORUGVnci6pLmV9ZrDm3iMgYMrK4zYymJSE2aj+3iMhpMrK4IT7nPtYX5pUjmnOLiJwoY4t7dD/36/pieRGRE2VscS+sLqWuqkRzbhGRU2RscUN81b1xzzF9/6SIyAkyuribGkJ09g/z0uEer6OIiGSMjC7u0f3cOm+JiMiojC7u+bNKOCNUykYVt4jIqIwubojPuTftPkZUc24RESALirupIUT3YIRdh7q9jiIikhEyvrh13hIRkZNlfHHXVhTTMLtMc24RkYSML26ApiUhnt1znEg05nUUERHPZUdxN4ToGYqw46Dm3CIiWVLc1YD2c4uIQJYU95xgMWfOKdecW0SELCluiO/nfm7PcYY15xaRPJc1xd3UEKIvHGXbgS6vo4iIeCqLijsx59Z+bhHJc1lT3KHyIpbWBjXnFpG8l3Rxm5nfzF4ws8fSGeitNC8J0bK3g3BEc24RyV+TWXF/AdiVriDJaGqoZmA4ytbWTi9jiIh4KqniNrM64ArgrvTGeWsXLg5hpjm3iOS3ZFfc/wLcAHg6o6gqK+ScuRVs3KPiFpH8NWFxm9n7gaPOuc0TPG6dmbWYWUtbW1vKAp6quSE+5x6KRNP2GiIimSyZFffFwAfNbC/wI+ASM7vv1Ac55+50zjU65xprampSHPNNTQ3VDEVivLhPc24RyU8TFrdz7ivOuTrnXD1wDfBb59zH0p5sHKNzbm0LFJE8lTX7uEdUlhZw7vwK7ecWkbw1qeJ2zv3OOff+dIVJVnNDiOf3dTI4rDm3iOSfrFtxQ/y8JeFIjOf3dXgdRURkxmVlcb99cTU+g43azy0ieSgri7uiuICVCyrZuPu411FERGZcVhY3xMclL+zvYCCsObeI5JfsLe4lIYajjs1vaM4tIvkla4v77fXV+H3Ght3tXkcREZlRWVvc5UUBVtVpzi0i+Sdrixvic+4t+zvpG4p4HUVEZMZkdXE3N4SIxBwtmnOLSB7J6uJurK+iwG86P7eI5JWsLu7SwgDn1c3SeUtEJK9kdXFDfM697UAXvZpzi0ieyPribl4SIhpzPLdHu0tEJD9kfXFfcEYVhX6fzs8tInkj64u7uMDP+Ys05xaR/JH1xQ3xOff2A110Dw57HUVEJO1yoribG0LEHDyroyhFJA/kRHGvXjSLwoDm3CKSH3KiuIsL/FywqEpzbhHJCzlR3BCfc+881E1nf9jrKCIiaZUzxd28JIRzsEn7uUUkx+VMcZ+3sJLiAp/GJSKS83KmuIsCfhrPqNYJp0Qk5+VMcQM0NVTz0uEejvdpzi0iuSunirt5SQiATRqXiEgOy6niXlU3i9JCv+bcIpLTcqq4C/w+GuurdSCOiOS0nCpuiM+5XznSS3vvkNdRRETSIueKu7khPufWuEREclXOFffKBZWUFwVU3CKSs3KuuAN+H2+vr9J+bhHJWTlX3BA/b8nrbX0c7R70OoqISMrlZHGP7OfW7hIRyUUTFreZFZvZs2a2xcx2mNlNMxFsOs6dX0mwOMBGfbGCiOSgQBKPGQIucc71mlkB8LSZ/cI5tzHN2abM7zMuXFytDyhFJCdNuOJ2cb2JmwWJH5fWVCnQ1BBiT3sfh7s05xaR3JLUjNvM/Gb2InAUeMI5t2mMx6wzsxYza2lra0t1zklrahiZc7d7nEREJLWSKm7nXNQ5dz5QB7zDzFaM8Zg7nXONzrnGmpqaVOectOXzKqgsKWDj65pzi0humdSuEudcJ/A74LK0pEkhX2LOrZ0lIpJrktlVUmNmsxLXS4D3AS+lO1gqNDWE2He8nwOdA15HERFJmWRW3POAp8xsK/Ac8Rn3Y+mNlRqj+7l1FKWI5JAJtwM657YCq2cgS8otrQ1SVVrAxt3HWHNBnddxRERSIiePnBzh8xlNDSGtuEUkp+R0cUN8zn2gc4D9x/u9jiIikhI5X9yac4tIrsn54j5rTjmzywt1+LuI5IycL24z48KGEBt2H8O5jD9SX0RkQjlf3BCfcx/qGuSNY5pzi0j2y4vibm7Q+blFJHfkRXEvqSmjJlikObeI5IS8KG4zozmxn1tzbhHJdnlR3BCfcx/tGWJ3e5/XUUREpiVvilv7uUUkV+RNcdeHSplbUaw5t4hkvbwpbjOjqaGajbuPa84tIlktb4ob4uOS9t4hXjvaO/GDRUQyVF4V9x+dVcMdH7+AmHO09QwRi2nlLSLZZ8LzceeKWMzROTDMzY/tpLVjgLqqEtavbWRpbRCfz7yOJyKStLxZcR/rC3PdvS20dsS/xqy1Y4Dr7m3hWF/Y42QiIpOTN8UdjkRHS3tEa8cA4UjUo0QiIlOTN8VdGPBTV1Vy0n11VSUUBvweJRIRmZq8Ke5QWSHr1zaOlnddVQm3rllFIG/+DYhIrsibDyd9PmNpbZBHrr+YcCRK1MGXfvwitZXFfOcv3uZ1PBGRpOXVetPnM2qCRSyoKmVRdSl/vLSGx7Ye4pfbD3sdTUQkaXlV3Kf69B8vYfm8Cv7up9vp7NfuEhHJDnld3AV+H7f92So6+sLc/Ngur+OIiCQlr4sb4Nz5lXz23Ut46PlWnnr5qNdxREQmlPfFDfD5S87krDnlfPXhbfQMDnsdR0TkLam4gaKAn1vXrOJI9yD/5xcveR1HROQtqbgTVi+q4lPvauCHm/bxzGvtXscRERmXivsEX7z0bBbPLuPGh7fSH454HUdEZEwq7hMUF/j5xw+vYv/xAW771ctexxERGZOK+xTvWFzNtc1ncM8ze9n8xnGv44iInEbFPYYbLjuH+ZUl/M8HtzI4rLMHikhmmbC4zWyhmT1lZrvMbIeZfWEmgnmprCjANz+8kt1tfXzryVe9jiMicpJkVtwR4EvOuWVAE/A5M1ue3ljee9dZNXykcSF3/n43W1s7vY4jIjJqwuJ2zh1yzj2fuN4D7AIWpDtYJvjqFcuYXV7IDQ9uJRyJeR1HRASY5IzbzOqB1cCmdITJNJUlBdxy9UpeOtzDd3/3mtdxRESASRS3mZUDDwF/7ZzrHuPX15lZi5m1tLW1pTKjp967rJarzp/Pd377GrsOnfa2RURmXFLFbWYFxEv7fufcw2M9xjl3p3Ou0TnXWFNTk8qMnvvaB85lVmkBNzy4lUhUIxMR8VYyu0oM+B6wyzn3z+mPlHmqygr5+pUr2Hagi/X/tcfrOCKS55JZcV8MfBy4xMxeTPxcnuZcGefylfP40xVzuf03r/B6W6/XcUQkjyWzq+Rp55w551Y5585P/Dw+E+EyzU1XnktpoZ8bHtxKNOa8jiMieUpHTk7CnGAxf//+5Wx+o4N7N+z1Oo6I5CkV9yRdvXoB71law62/fJl9x/q9jiMieUjFPUlmxi0fWknAZ9z40Fac08hERGaWinsK5lWW8NUrlrFh9zEeeHa/13FEJM+ouKfomrcv5OIzQ9zy+C4Odg54HUdE8oiKe4rMjG9+aBXRmOOrj2zTyEREZoyKexoWVpdy42VL+d3LbTz8/AGv44hInlBxT9Pa5noaz6jipkd3cLR70Os4IpIHVNzT5PMZt65ZxVAkxv/6f9s1MhGRtFNxp0BDTTlfvPRsfr3zCD/fdsjrOCKS41TcKfLJdy5mVV0lX/vpDo73hb2OIyI5TMWdIgG/j1vXrKJ7cJh7/rCbtp4hDnT009YzREznNRGRFAp4HSCXnDO3gpuvXMHC6lKu/u4faO0YoK6qhPVrG1laG8TnM68jikgO0Io7xd5zzhxufGgrrR3xg3JaOwa47t4Wjml8IiIpouJOsUg0NlraI1o7BghHoh4lEpFco+JOscKAn7qqkpPuq6sqoTDg9yiRiOQaFXeKhcoKWb+2cbS866pKuHXNKroGNCoRkdTQh5Mp5vMZS2uDPHL9xYQjUWIOvv7oDjbsPs5d1zbS1BDyOqKIZDmtuNPA5zNqgkUsqCplYXUpX79qBbUVRVx797P8ZucRr+OJSJZTcc+AeZUl/MdnLmLp3CCfvm8zj7zQ6nUkEcliKu4ZUl1WyA+va+LCxdX8zY+38P0/7PE6kohkKRX3DCovCnD3J97Onyyv5aZHd3L7E6/opFQiMmkq7hlWXODnux99G2suqONbT77KTY/u1CHxIjIp2lXigYDfx60fXsWskgLuenoPnf1hbvuz8yjw689REZmYitsjPp/xt1cso6qskNt+9TLdgxG++9G3UVygA3VE5K1piechM+Nz7zmTf7hqBU+9fJS133uW7sFhr2OJSIZTcWeAjzWdwbevWc3z+zq45o6NtPUMeR1JRDKYijtDfOC8+dx1bSO723v58zs20NrR73UkEclQKu4M8u6lc7jvkxdyrHeINf+2gVeP9HgdSUQykIo7wzTWV/PjTzcTiTn+/I4NbNnf6XUkEckwKu4MtGxeBQ99tpny4gB/sX4jz7zW7nUkEckgKu4MdUaojAc/cxELqkr4xPef45fbD3sdSUQyxITFbWZ3m9lRM9s+E4HkTbUVxfzk082cu6CC6+/fzE+e2+91JBHJAMmsuO8BLktzDhnHrNJC7v/UhVx85mxueGgr63+/2+tIIuKxCYvbOfd74PgMZJFxlBYGuOvaRq5YOY9vPL6L2371kk5OJZLHdMh7ligK+Pn2f1tNRUmAf33qdcoKA6xprGM4EqMw4CdUVojPZ17HFJEZkLLiNrN1wDqARYsWpepp5QR+n3HL1StZNi/IkpogH/ruM7R2DFBXVcL6tY0srQ2qvEXyQMp2lTjn7nTONTrnGmtqalL1tHIKM+NPV8znxoe20toxAEBrxwDX3dvCa0d72dvep9PEiuQ4jUqyUDgSHS3tEa0dA3T0h/nInRsJFgU4d0EFK+ZXsrKuknPnV9Iwu0yrcZEcMWFxm9kDwLuB2WbWCnzNOfe9dAeT8RUG/NRVlZxU3nVVJcytLOabH1rJ9oNdbDvQzb0b3yAciQFQVuhn+fwKViyoHC30htllBHQOcJGsY+nYndDY2OhaWlpS/rwSF4s5Xj7Sw3X3trzljHs4GuO1o71sP9AV/znYzc6D3QwMRwEoLvCxfF6izBOFflZt+egXOsRijmN9YcKRqD4AFUkzM9vsnGtM6rEq7uw01VKNxhy723rZdqCL7Qe62X6gix0Hu+gLx8u8MOBj2dwgl507l+YzQ3z+hy/oA1CRGaDilkmJxRx7jvWNrsy3HejiU+9czP9+dOdp45hHrr+YmmCRh2lFctNkilsfTgo+n7GkppwlNeVcef4CAA509I/5AejBzgF2HermnWfO1spbxCP6ZErGNPIB6Inqqkpo7x1i7d3P8r7b/5N7N+yldyjiTUCRPKbiljGFygpZv7ZxtLxHZtwXnxni9o+cR7AowN//dAfNtzzJTY/uYG97n8eJRfKHZtwyrok+AH1hXwf3PLOXn289RNQ53rN0DtdeVM+7NEYRmTR9OCkz6mj3IPdv2sf9m/bR3jtEQ00Zn7iong+9rY7yIn2MIpIMFbd4YigS5fFth7jnD3vZ0tpFsCh+Iqxrm+upn13mdTyRjKbiFs+NjFEe33aISExjFJGJqLglY2iMIpIcFbdknKFIlF9sO8z3n9nLlv2dJ41RFlWX6tB6yXs6AEcyTlHAz1WrF3DV6gW8sK+DHzyzl/s2vsGL+zr5yuXn8MWfbNGh9SJJ0opbPHO0e5BDXYN87ofPn3Zo/T9+eBUPbm5lbmUx8yqLqa2IX86tLGZ2WdGkS10nzJJMpxW3ZIU5FcUMR2NjHlpfWujnub3HOdI9yHD05MVFwGfUVsRLfG5lMfMS1+dVljC3soi5lSXMCRaddJbDZM6mKJItVNziqfHOLV5XVcrTN14yulI+klidH+4aSFzGb+862M2Tu44wOBw76XnNoKa8iHmVxXzl8mX8j//Ycto3BumEWZKtVNziqZFD609dDYfKCoH4CbBqgkXUBItYsaByzOdwztE9EOFQ95ulPvJzqHuQAr9v3BNmPfDsPlbVVXJe3SyqEq8pkulU3OIpn89YWhvkkesvnvL82cyoLC2gsrSAc+ZWnPbrbT1DY67qO/rD3P6bVxj5mGdhdQmr6mZxXl0lq+pmsWJBpbYsSkbSh5OS895qxt0XjrDtQBdbW7vY2trJlv1dHOiMF7wZnFlTHi/zhfEyXzYvSFHA7/E7klykfdwip5jMrpL23iG2tXaxpbVztNDbe8MAFPiNc+ZWjI5XVi2s5Kw5Qfw+084VmRYVt0gKOec42DXI1v2dbEkU+bbWLnoS5yIvKfBz1ep5rLlgEV/4kb7qTaZGxS2SZiNf9zYyXnnfsjl8+eFtp83Rb//I+by4r5Oz5wZZWhuktqIIMxW5nE77uEXS7MSve7t6dd24X/UG8I3Hd43eV1EcYOncIGfXBkcvz64NUq0dLTIJKm6RFBhvP3p9qIwX/u5SXjnSwytHenj5SA+vHO7l0S0HuX/Tm1/7VhMsYmntSKGXc3ZtkLNqg2PuapmJWbrm9ZlNoxKRFJjs0ZnOOY72DPHy4UShJy5fOdLLwHB09HF1VSXxQk+MWlYuqGQoEmPdv6fvKFAdaeoNzbhFPJCKVWos5mjtGIivzE8o9NfbehmOOu74+AXc/NjO01b2t65ZxV3/tYeAzyjw+wj4jYDPR2EgfhnwJ+73GQG/j4KRS7/F/5mAj4LE486dX8knf/Dcaa/x8PUXMSdYnLJ/X3IyzbhFPDBylOd0n2NRqJRFoVIuXV47ev9wNDb6hcxjzdKLC/wc7RlkOOIYjsWIRB2RaIzhWPwyEj3h/thbL9Z+vK5pzNfY09bH5d96mtnlhcwuLyJ04mVZEbODhYTKipgdLCJUVkhxwVvvd9c4ZupU3CJZoMDv46za4LhHgS6sKuWxv3xXUs/lXLy8h6MxhhMFP3I7EnX4fTbmaxQX+Ll0+RzaesIc6xti375+2nuH6A9Hx3ydYFGAUHkhofIiZo9exq831JRRWVzAZ+9/Pu3jmFz8A0KjEpEsMhPz58m+Rn84wrHeMO29Q7T3hjnWO/Tm9b4w7T1DHOuL3+7oD+Mc4458vvaB5Xz5oW0UF/gpKfRTXOCjpMAfv524b+T2m/f5xrgvfn1WSQEDw1E+c9/mjJ/Xa8YtksOyeVdJJBrjeH+Y/qEo7/6n35326z//q3fyw037GBiOMjQcY2A4ykA4ysBwlMHEz8h9g8MxwtHY6S9ygvH+gLj9I+fz9KvtLKwuZWFVCQurS6mtKMbvYZlrxi2Sw1IxS/fqNQJ+H3OCxbQx9shnTrCYb1y9Munni8bcKWUeL/SBxH0LZhWPu7/+2799lRPXrQV+Y8GseInXVZXGR1AnFHuorHDcg6dmehyj4haRGTfR6XyT5fcZZUUBysY5i+N4nwnUh8p46ebLONg5yP7j/ezv6Gf/8QH2d/TTeryfXx08zPG+8EnPVVroH/08IV7u8UI/s6acoUiUdf8+c+MYjUpExBMzNfKZ6mcCfUORNwv9ePzI2Pjt+PXexLlqxhvHTPaLOlI+KjGzy4BvAX7gLufcN5NOIyIyhpka+Uz1fO9lRQHOmVsx5jnenXN09g+zv6OfkgL/mOOYcGTs3TapMGFxm5kf+FfgUqAVeM7Mfuac25m2VCIiKZKOPyDMjKqyQqrKCscdxxSm8bztviQe8w7gNefcbudcGPgRcGXaEomIZJGReX1dVQnAlOf1k5HMqGQBsP+E263AhemJIyKSXVLx9XuTlUxxj/Xqp32iaWbrgHUAixYtmmYsEZHsMRPz+pNeL4nHtAILT7hdBxw89UHOuTudc43OucaamppU5RMRkVMkU9zPAWeZ2WIzKwSuAX6W3lgiIjKeCUclzrmImX0e+BXx7YB3O+d2pD2ZiIiMKal93M65x4HH05xFRESSkMyoREREMkhaDnk3szbgjZQ/cerMBtq9DpEiei+ZJ1feB+i9zKQznHNJ7exIS3FnOjNrSfacAJlO7yXz5Mr7AL2XTKVRiYhIllFxi4hkmXwt7ju9DpBCei+ZJ1feB+i9ZKS8nHGLiGSzfF1xi4hkrbwqbjNbaGZPmdkuM9thZl/wOtN0mJnfzF4ws8e8zjIdZjbLzB40s5cS/22avc40VWb2N4nfW9vN7AEzK/Y6U7LM7G4zO2pm20+4r9rMnjCzVxOXVV5mTNY47+W2xO+xrWb2iJnN8jLjdORVcQMR4EvOuWVAE/A5M1vucabp+AKwy+sQKfAt4JfOuXOA88jS92RmC4C/AhqdcyuInyLiGm9TTco9wGWn3Pdl4Enn3FnAk4nb2eAeTn8vTwArnHOrgFeAr8x0qFTJq+J2zh1yzj2fuN5DvCAWeJtqasysDrgCuMvrLNNhZhXAHwHfA3DOhZ1znd6mmpYAUGJmAaCUMc6kmamcc78Hjp9y95XADxLXfwBcNaOhpmis9+Kc+7VzLpK4uZH4mU6zUl4V94nMrB5YDWzyNsmU/QtwAxDzOsg0NQBtwPcTY5+7zKzM61BT4Zw7APwTsA84BHQ5537tbappq3XOHYL4wgeY43GeVPnvwC+8DjFVeVncZlYOPAT8tXOu2+s8k2Vm7weOOuc2e50lBQLA24B/c86tBvrInr+OnyQx/70SWAzMB8rM7GPeppJTmdnfEh+b3u91lqnKu+I2swLipX2/c+5hr/NM0cXAB81sL/HvAL3EzO7zNtKUtQKtzrmRv/k8SLzIs9H7gD3OuTbn3DDwMHCRx5mm64iZzQNIXB71OM+0mNm1wPuBj7os3gudV8VtZkZ8lrrLOffPXueZKufcV5xzdc65euIffv3WOZeVKzvn3GFgv5ktTdz1XmCnh5GmYx/QZGalid9r7yVLP2g9wc+AaxPXrwV+6mGWaTGzy4AbgQ865/q9zjMdeVXcxFeqHye+Qn0x8XO516GEvwTuN7OtwPnALR7nmZLE3xoeBJ4HthH//ytrjtYzsweADcBSM2s1s08C3wQuNbNXgUsTtzPeOO/lO0AQeCLx//7/9TTkNOjISRGRLJNvK24Rkayn4hYRyTIqbhGRLKPiFhHJMipuEZEso+IWEckyKm4RkSyj4hYRyTL/HyXRqHHPr+x/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization\n",
    "\n",
    "sns.lineplot(x = range(1,14), y = eig_vals_s, marker ='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.373, 0.561, 0.677, 0.75 , 0.813, 0.857, 0.895, 0.923, 0.944,\n",
       "       0.963, 0.979, 0.992, 1.   ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Principal Component\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "np.cumsum(pca.explained_variance_ratio_)\n",
    "# eigenvalue가 1 이상인 3개의 component로는 설명력이 떨어지므로, 4개 선정.\n",
    "# 네번째 값도 1에 가까운 값을 가지고 있으므로 유의할 것. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:467: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:511: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:511: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:467: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:511: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:511: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:467: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:511: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:511: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:467: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:511: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:511: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:467: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit PCA Train : 0.9758064516129032\n",
      "Logit PCA Test : 0.9629629629629629\n",
      "LDA PCA Train : 0.967741935483871\n",
      "LDA PCA Test : 0.9814814814814815\n",
      "QDA PCA Train : 0.9919354838709677\n",
      "QDA PCA Test : 0.9629629629629629\n",
      "Logit 1 Poly PCA Train : 0.9758064516129032\n",
      "Logit 1 Poly PCA Test : 0.9629629629629629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:511: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:511: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:467: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit 2 Poly PCA Train : 1.0\n",
      "Logit 2 Poly PCA Test : 0.9074074074074074\n",
      "Logit 3 Poly PCA Train : 1.0\n",
      "Logit 3 Poly PCA Test : 0.9629629629629629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:511: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:511: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    }
   ],
   "source": [
    "# Model Selection\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "Logit_pca = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('pca', PCA(n_components=4)),\n",
    "                     ('LR', LR(solver='sag', max_iter=10000, multi_class='auto'))])\n",
    "\n",
    "Logit_pca.fit(X_train, y_train)\n",
    "\n",
    "print('Logit PCA Train :', Logit_pca.score(X_train, y_train))\n",
    "print('Logit PCA Test :', Logit_pca.score(X_test, y_test))\n",
    "\n",
    "# LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "LDA_pca = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('pca', PCA(n_components=4)),\n",
    "                     ('LDA', LDA())])\n",
    "LDA_pca.fit(X_train, y_train)\n",
    "\n",
    "print('LDA PCA Train :', LDA_pca.score(X_train, y_train))\n",
    "print('LDA PCA Test :', LDA_pca.score(X_test, y_test))\n",
    "\n",
    "# QDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "\n",
    "QDA_pca = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('pca', PCA(n_components=4)),\n",
    "                     ('QDA', QDA())])\n",
    "QDA_pca.fit(X_train, y_train)\n",
    "\n",
    "print('QDA PCA Train :', QDA_pca.score(X_train, y_train))\n",
    "print('QDA PCA Test :', QDA_pca.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# feature extraction(logistic regression with polynomial feature)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "for i in range(1,4):\n",
    "    Logit_poly_pca =  Pipeline([('scaler', StandardScaler()),\n",
    "                                ('pca', PCA(n_components=4)),\n",
    "                                ('poly', PolynomialFeatures(degree=i, include_bias=False)),\n",
    "                                ('LR', LR(solver='sag', max_iter=10000, multi_class='auto'))])\n",
    "    Logit_poly_pca.fit(X_train, y_train)\n",
    "    \n",
    "    print('Logit %d Poly PCA Train :' %i, Logit_poly_pca.score(X_train, y_train))\n",
    "    print('Logit %d Poly PCA Test :' %i, Logit_poly_pca.score(X_test, y_test))\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.014</td>\n",
       "      <td>1.635</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.282</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.965</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>-1.859</td>\n",
       "      <td>-0.935</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.395</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.335</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.037</td>\n",
       "      <td>1.410</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.305</td>\n",
       "      <td>-0.592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1      2      3      4      5      6    7    8    9    10   11  \\\n",
       "0  1.014  1.635 -0.973 -0.188  0.675  0.282 -0.224  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.965 -0.411 -1.859 -0.935 -0.337  0.218 -0.139  0.0  0.0  0.0  0.0  0.0   \n",
       "2  1.395 -0.495  0.211  0.645  0.470  0.110  0.126  0.0  0.0  0.0  0.0  0.0   \n",
       "3  2.335  0.016  0.223  0.008  0.941  0.205  0.215  0.0  0.0  0.0  0.0  0.0   \n",
       "4 -0.037  1.410  0.093  0.744  0.434  0.305 -0.592  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "    12  \n",
       "0  0.0  \n",
       "1  0.0  \n",
       "2  0.0  \n",
       "3  0.0  \n",
       "4  0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import FactorAnalysis as FA\n",
    "\n",
    "fa = FA()\n",
    "X_fa = fa.fit_transform(X)\n",
    "\n",
    "df_fa = pd.DataFrame(data=X_fa)\n",
    "df_fa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'factor_analyzer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c5db62fcd55a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfactor_analyzer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFactorAnalyzer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfa_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFactorAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrotation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'varimax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfa_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfa_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_eigenvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'factor_analyzer'"
     ]
    }
   ],
   "source": [
    "from factor_analyzer import FactorAnalyzer\n",
    "\n",
    "fa_1 = FactorAnalyzer(rotation='varimax')\n",
    "fa_1.fit(X)\n",
    "ev, v = fa_1.get_eigenvalues()\n",
    "xvals = range(1, X.shape[1]+1)\n",
    "\n",
    "plt.scatter(xvals, ev)\n",
    "plt.plot(xvals, ev)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Factor')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit FA Train: 1.0\n",
      "Logit FA Test : 1.0\n",
      "LDA FA Train : 0.9838709677419355\n",
      "LDA FA Test : 1.0\n",
      "QDA FA Train: 1.0\n",
      "QDA FA Test: 0.9814814814814815\n",
      "Logit 1 Poly FA Train : 1.0\n",
      "Logit 1 Poly FA Test : 0.9444444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:467: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:511: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:511: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:467: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:511: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:511: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:467: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit 2 Poly FA Train : 1.0\n",
      "Logit 2 Poly FA Test : 0.9259259259259259\n",
      "Logit 3 Poly FA Train : 1.0\n",
      "Logit 3 Poly FA Test : 0.9629629629629629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:511: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:511: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logit_fa = Pipeline([('fa', FA(n_components = 7)),\n",
    "                    ('estimator', LR(solver='sag', max_iter=10000, multi_class='auto'))])\n",
    "Logit_fa.fit(X_train, y_train)\n",
    "Logit_fa.score(X_test, y_test)\n",
    "\n",
    "print('Logit FA Train:', Logit_fa.score(X_train, y_train))\n",
    "print('Logit FA Test :', Logit_fa.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# LDA\n",
    "\n",
    "LDA_fa = Pipeline([('fa', FA(n_components = 7)), ('estimator', LDA())])\n",
    "LDA_fa.fit(X_train, y_train)\n",
    "\n",
    "print('LDA FA Train :', LDA_fa.score(X_train, y_train))\n",
    "print('LDA FA Test :', LDA_fa.score(X_test, y_test))\n",
    "\n",
    "# QDA\n",
    "\n",
    "QDA_fa = Pipeline([('fa', FA(n_components = 7)), ('estimator', QDA())])\n",
    "QDA_fa.fit(X_train, y_train)\n",
    "\n",
    "print('QDA FA Train:', QDA_fa.score(X_train, y_train))\n",
    "print('QDA FA Test:', QDA_fa.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# feature extraction\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "for i in range(1,4):\n",
    "    Logit_poly_fa =  Pipeline([('scaler', StandardScaler()),\n",
    "                                ('FA', FA(n_components=7)),\n",
    "                                ('poly', PolynomialFeatures(degree=i, include_bias=False)),\n",
    "                                ('LR', LR(solver='sag', max_iter=10000, multi_class='auto'))])\n",
    "    Logit_poly_fa.fit(X_train, y_train)\n",
    "    \n",
    "    print('Logit %d Poly FA Train :' %i, Logit_poly_fa.score(X_train, y_train))\n",
    "    print('Logit %d Poly FA Test :' %i, Logit_poly_fa.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
